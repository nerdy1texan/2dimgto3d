!git clone https://github.com/tensorflow/tpu/    #clones trained tensor-flow models 

#import libraries to use
from IPython import display #ipy for displaying 
from PIL import Image #PIL for image imports 
import numpy as np 
%tensorflow_version 1.x   
#use ver 2 in Colab; doesn't support Ver 1 #Mask RCNN is based on ver 1
import tensorflow as tf
import sys
sys.path.insert(0, 'tpu/models/official')
sys.path.insert(0, 'tpu/models/official/mask_rcnn') 
#we will import models from official tensorflor repository
import coco_metric
#we are using classes from the coco dataset
from mask_rcnn.object_detection import visualization_utils
#all display inside bounding box is because of this library

#importing COCO dataset classes since the model is based on COCO dataset

ID_MAPPING = {
    1: 'person',
    2: 'bicycle',
    3: 'car',
    4: 'motorcycle',
    5: 'airplane',
    6: 'bus',
    7: 'train',
    8: 'truck',
    9: 'boat',
    10: 'traffic light',
    11: 'fire hydrant',
    13: 'stop sign',
    14: 'parking meter',
    15: 'bench',
    16: 'bird',
    17: 'cat',
    18: 'dog',
    19: 'horse',
    20: 'sheep',
    21: 'cow',
    22: 'elephant',
    23: 'bear',
    24: 'zebra',
    25: 'giraffe',
    27: 'backpack',
    28: 'umbrella',
    31: 'handbag',
    32: 'tie',
    33: 'suitcase',
    34: 'frisbee',
    35: 'skis',
    36: 'snowboard',
    37: 'sports ball',
    38: 'kite',
    39: 'baseball bat',
    40: 'baseball glove',
    41: 'skateboard',
    42: 'surfboard',
    43: 'tennis racket',
    44: 'bottle',
    46: 'wine glass',
    47: 'cup',
    48: 'fork',
    49: 'knife',
    50: 'spoon',
    51: 'bowl',
    52: 'banana',
    53: 'apple',
    54: 'sandwich',
    55: 'orange',
    56: 'broccoli',
    57: 'carrot',
    58: 'hot dog',
    59: 'pizza',
    60: 'donut',
    61: 'cake',
    62: 'chair',
    63: 'couch',
    64: 'potted plant',
    65: 'bed',
    67: 'dining table',
    70: 'toilet',
    72: 'tv',
    73: 'laptop',
    74: 'mouse',
    75: 'remote',
    76: 'keyboard',
    77: 'cell phone',
    78: 'microwave',
    79: 'oven',
    80: 'toaster',
    81: 'sink',
    82: 'refrigerator',
    84: 'book',
    85: 'clock',
    86: 'vase',
    87: 'scissors',
    88: 'teddy bear',
    89: 'hair drier',
    90: 'toothbrush',
}
category_index = {k: {'id': k, 'name': ID_MAPPING[k]} for k in ID_MAPPING}
#category index creates a map between ID and class 

#image upload: we can either use any image from URL or upload our own image and link it with a URL

!wget https://upload.wikimedia.org/wikipedia/commons/thumb/0/08/Kitano_Street_Kobe01s5s4110.jpg/2560px-Kitano_Street_Kobe01s5s4110.jpg -O test.jpg 
#stores image in test.jpg
image_path = 'test.jpg'

with open(image_path, 'rb') as f:
  np_image_string = np.array([f.read()])
#storing in an object and opening with a numpy image string
  
image = Image.open(image_path)
width, height = image.size
np_image = np.array(image.getdata()).reshape(height, width, 3).astype(np.uint8) 
#setting height and width and 3 RGB channels

display.display(display.Image(image_path, width=1024))
#after converting the image into a numpy image, we are displaying it here

#creating a tensorflow session
use_tpu = True #@param {type:"boolean"} 
#connect TPU or a normal CPU
if use_tpu:
  import os
  import pprint

  assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'
  TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']
  print('TPU address is', TPU_ADDRESS)

  session = tf.Session(TPU_ADDRESS, graph=tf.Graph())
  # For TF ver 2, use tf.compat.v1.Session instead of tf.Session to get the same behavior
  print('TPU devices:')
  pprint.pprint(session.list_devices())
else:
  session = tf.Session(graph=tf.Graph())
  
  #loading the pre-trained model on the COCO dataset
  saved_model_dir = 'gs://cloud-tpu-checkpoints/mask-rcnn/1555659850' #@param {type:"string"}
_ = tf.saved_model.loader.load(session, ['serve'], saved_model_dir)

#saved model directory is gs://cloud-tpu-checkpoints/mask-rcnn/1555659850
#use tf.compat.v1.saved_model.loader.load for ver2
#refer to TF2 symbols map 
#https://docs.google.com/spreadsheets/d/1FLFJLzg7WNP6JHODX5q8BDgptKafq_slHpnHVbJIteQ/edit#gid=0

num_detections, detection_boxes, detection_classes, detection_scores, detection_masks, image_info = session.run(     #session.run does a forward pass of the image in the neural network
    ['NumDetections:0', 'DetectionBoxes:0', 'DetectionClasses:0', 'DetectionScores:0', 'DetectionMasks:0', 'ImageInfo:0'], #array of all outputs including detection masks and image info
    feed_dict={'Placeholder:0': np_image_string})  
    
#instance object segmentation retriving predictions: boundry boxes, class names, masks
num_detections = np.squeeze(num_detections.astype(np.int32), axis=(0,)) #using squeeze function from numpy 
detection_boxes = np.squeeze(detection_boxes * image_info[0, 2], axis=(0,))[0:num_detections] #numpy arrays to be displayed with the visualization function
detection_scores = np.squeeze(detection_scores, axis=(0,))[0:num_detections] 
detection_classes = np.squeeze(detection_classes.astype(np.int32), axis=(0,))[0:num_detections]
instance_masks = np.squeeze(detection_masks, axis=(0,))[0:num_detections]
ymin, xmin, ymax, xmax = np.split(detection_boxes, 4, axis=-1) #xmax ymax for our boundry boxes 
processed_boxes = np.concatenate([xmin, ymin, xmax - xmin, ymax - ymin], axis=-1) #numpy concatenation of top-left and botton right corners to be concatenated to process boxes
segmentations = coco_metric.generate_segmentation_from_masks(instance_masks, processed_boxes, height, width) #storing secmentations in a variable for intance segementation
#also adjusting the segmentations to the height and width of an image

#visualizing the detected results 
max_boxes_to_draw = 50   #@param {type:"integer"}  #maximum number of boxes that we want to draw is set as 50
min_score_thresh = 0.1    #@param {type:"slider", min:0, max:1, step:0.01} 
#if we want to be more cofident in image classes that we predict, then we can increase this threshold to 0.5 or 0.8 depending on the minimum confidence level we want to set for each prediction. 
#lower the threshold if the algorithm is not detecting any images since the threshold is so high

image_with_detections = visualization_utils.visualize_boxes_and_labels_on_image_array(  #creating an image-detactions array then displaying it as image.prl 
    np_image, 
    detection_boxes,
    detection_classes,
    detection_scores,
    category_index,
    instance_masks=segmentations,
    use_normalized_coordinates=False,
    max_boxes_to_draw=max_boxes_to_draw,
    min_score_thresh=min_score_thresh)

#passing all the squeezed outputs from the neural network to image array

output_image_path = 'test_results.jpg' #creating image from array and saving into this output image path 
Image.fromarray(image_with_detections.astype(np.uint8)).save(output_image_path) 
display.display(display.Image(output_image_path, width=1024))
segmentations = coco_metric.generate_segmentation_from_masks(instance_masks, processed_boxes, height, width)


